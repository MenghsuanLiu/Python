{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每日主題知識點\n",
    "● 多元線性回歸\n",
    "\n",
    "甚麼是多元線性回歸\n",
    "● 多元線性回歸\n",
    "還記得前兩章我們用單變數來做線性回歸?相信大家都覺得太無趣了，這麼簡單用手算就好了。所以今天我們要往前邁進一步。\n",
    "\n",
    "● 今日目標\n",
    "如果我們一次拿多個變數來處理，包括資料整理、設定損失函數與梯度下降等等，情況會變得非常複雜!今天我們就專注於處理多個變數的線性回歸吧!\n",
    "\n",
    "● 我們本日一樣採用糖尿病資料集，但除了年齡，再多加入一筆 BMI 數值(在數據中名字為 bmi)。\n",
    "\n",
    "● 比起兩天前的程式碼，獲得兩筆變數的數據(糖尿病數據集)，只不過多了一個步驟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "data, yt = diabetes.data, diabetes.target  # data 每一筆資料為一間房子 yt為對應到data的疾病概況\n",
    "feature_names = diabetes.feature_names\n",
    "print('輸入資料', data.shape, yt.shape)\n",
    "print('資料的特徵名', feature_names)\n",
    "\n",
    "# 單看 age(年齡) 這筆特徵\n",
    "x_data = data[:, feature_names.index('age')].reshape(-1, 1)\n",
    "print('整理後', x_data.shape)\n",
    "\n",
    "# 在 x 向量第 1 個位置中加入虛擬變數\n",
    "x = np.insert(x_data, 0, 1.0, axis=1)\n",
    "print('加入虛擬變數後', x.shape)\n",
    "\n",
    "# 整理完後的資料長相\n",
    "print(x.shape)\n",
    "\n",
    "# 看看前五位病人之年齡(加上虛擬變數)如何對應疾病概況\n",
    "print(x[:5,:])\n",
    "print(yt[:5])\n",
    "輸出如下:\n",
    "輸入資料 (442, 10) (442,)\n",
    "特徵名 ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "整理後 (442, 1)\n",
    "加入虛擬變數後 (442, 2)\n",
    "(442, 2)\n",
    "[[ 1. 0.03807591]\n",
    "[ 1. -0.00188202]\n",
    "[ 1. 0.08529891]\n",
    "[ 1. -0.08906294]\n",
    "[ 1. 0.00538306]]\n",
    "\n",
    "● 我們這次再追加一個變數(BMI):\n",
    "\n",
    "# 用 hstack() 追加列（BMI）\n",
    "x_add = data[:,feature_names.index('bmi')].reshape(-1, 1)\n",
    "x2 = np.hstack((x, x_add))\n",
    "print(x2.shape)\n",
    "\n",
    "# 輸出輸入資料（含虛擬資料）\n",
    "print(x2[:5,:])\n",
    "\n",
    "# 輸出實際值\n",
    "print(yt[:5])\n",
    "輸出如下:\n",
    "(442, 3)\n",
    "[[ 1. 0.03807591 0.06169621]\n",
    "[ 1. -0.00188202 -0.05147406]\n",
    "[ 1. 0.08529891 0.04445121]\n",
    "[ 1. -0.08906294 -0.01159501]\n",
    "[ 1. 0.00538306 -0.03638469]]\n",
    "\n",
    "實作多元線性迴歸(I)\n",
    "● 首先，了解必須清楚我們想採取的的模型長相(如圖1)，以及預測函數、資料集長相(如圖2)與損失函數還有優化演算法。\n",
    "\n",
    "● 模型長相\n",
    "首先要確定輸入及輸出。\n",
    "多元線性迴歸.png\n",
    "圖1\n",
    "\n",
    "● 預測函數\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "● 資料長相\n",
    "image\n",
    "圖2\n",
    "\n",
    "● 損失函數\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "這裡的上標 m 代表第 m 筆資料(如圖1中左邊的index)。\n",
    "這裡已經幫大家算好這個損失函數的偏微分，如下\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "這裡 \n",
    "  是第 m 筆數據之誤差(預測值\n",
    "真實值)\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "● 優化演算法\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "\n",
    " \n",
    " \n",
    " \n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "這裡\n",
    "為學習率(learning rate，可參照機器學習馬拉松)。\n",
    "\n",
    "實作多元線性回歸(II)\n",
    "● 我們用程式驗證看看上一頁的數學分析吧!\n",
    "\n",
    "● 預測函數\n",
    "\n",
    "# 以預測函數 (1, x) 之值計算預測值 yp\n",
    "def pred(x, w):\n",
    "    return(x @ w)\n",
    "● 初始化\n",
    "\n",
    "# 初始化處理\n",
    "\n",
    "# 資料樣本總數\n",
    "M = x2.shape[0]\n",
    "\n",
    "# 輸入資料之維數（含虛擬變數）\n",
    "D = x2.shape[1]\n",
    "\n",
    "# 迭代運算次數\n",
    "iters = 50000\n",
    "# iters = 2000\n",
    "\n",
    "# 學習率\n",
    "alpha = 0.01\n",
    "# alpha = 0.001\n",
    "\n",
    "# 權重向量的初始值（預設全部為 1）\n",
    "w = np.ones(D)\n",
    "\n",
    "# 記錄評估結果用（僅記錄損失函數值）\n",
    "history = np.zeros((0,2))\n",
    "● 迭代運算\n",
    "\n",
    "# 迭代運算\n",
    "for k in range(iters):\n",
    "    \n",
    "    # 計算預測值（7.8.1）\n",
    "    yp = pred(x2, w)\n",
    "    \n",
    "    # 計算誤差（7.8.2）\n",
    "    yd = yp - yt\n",
    "    \n",
    "    # 梯度下降法的實作（7.8.4）\n",
    "    w = w - alpha * (x2.T @ yd) / M\n",
    "    \n",
    "    # 繪製學習曲線所需資料之計算與儲存\n",
    "    if ( k % 100 == 0):\n",
    "        # 計算損失函數值（7.6.1）\n",
    "        loss = np.mean(yd ** 2) / 2\n",
    "        # 記錄計算結果\n",
    "        history = np.vstack((history, np.array([k, loss])))\n",
    "        # 顯示畫面\n",
    "#        print( \"iter = %d  loss = %f\" % (k, loss))\n",
    "● 查看輸出\n",
    "\n",
    "# 損失函數的初始值、最終值\n",
    "print('損失函數初始值: %f' % history[0,1])\n",
    "print('損失函數最終值: %f' % history[-1,1])\n",
    "輸出結果:\n",
    "損失函數初始值: 14382.773907\n",
    "損失函數最終值: 2023.238005\n",
    "\n",
    "● 調整超參數後，運行迭代及查看結果 (把學習率降為\n",
    "，把迭代次數降為\n",
    ")，記得重新迭代，要重設權重值(已被更新過)喔!\n",
    "\n",
    "# 降低迭代次數\n",
    "iters = 2000\n",
    "\n",
    "# 降低學習率\n",
    "alpha = 0.001\n",
    "\n",
    "# 權重向量的初始值（預設全部為 1）\n",
    "w = np.ones(D)\n",
    "\n",
    "# 記錄評估結果用（僅記錄損失函數值）\n",
    "history = np.zeros((0,2))\n",
    "\n",
    "# 迭代運算\n",
    "for k in range(iters):\n",
    "    \n",
    "    # 計算預測值（7.8.1）\n",
    "    yp = pred(x2, w)\n",
    "    \n",
    "    # 計算誤差（7.8.2）\n",
    "    yd = yp - yt\n",
    "    \n",
    "    # 梯度下降法的實作（7.8.4）\n",
    "    w = w - alpha * (x2.T @ yd) / M\n",
    "    \n",
    "    # 繪製學習曲線所需資料之計算與儲存\n",
    "    if ( k % 100 == 0):\n",
    "        # 計算損失函數值（7.6.1）\n",
    "        loss = np.mean(yd ** 2) / 2\n",
    "        # 記錄計算結果\n",
    "        history = np.vstack((history, np.array([k, loss])))\n",
    "        # 顯示畫面\n",
    "#        print( \"iter = %d  loss = %f\" % (k, loss))\n",
    "\n",
    "# 損失函數的初始值、最終值\n",
    "print('損失函數初始值: %f' % history[0,1])\n",
    "print('損失函數最終值: %f' % history[-1,1])\n",
    "輸出結果:\n",
    "損失函數初始值: 14382.773907\n",
    "損失函數最終值: 3207.520633\n",
    "\n",
    "● 可視化我們的訓練過程吧\n",
    "\n",
    "# 繪製學習曲線（第一組數除外）\n",
    "plt.plot(history[:,0], history[:,1])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "知識點總整理\n",
    "● 多元線性回歸\n",
    "\n",
    "多個變數組成的線性回歸問題，在計算上較複雜，建議同學熟悉前一天的數學概念，對深度學習也非常有用。\n",
    "\n",
    "延伸閱讀\n",
    "● 在此分享有關學習率的文章，建議本課程與機器學習馬拉松同時進行，更能讓自己在數學簡單分析能力與程式能力做結合。\n",
    "\n",
    "https://www.plob.org/article/21083.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
