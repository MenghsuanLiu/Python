# %%
import maximal
from maximal.layers import TransformerLayer, GPTLayer
import tensorflow as tf
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding
from tensorflow.keras.layers import LSTM
from tensorflow.keras.datasets import imdb

# import pktf

# -*- coding: utf-8 -*-
"""sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j0vDhAZX7Ni_sdCDb0C1veMtW3FEXlRD

# A Transformer Neural Network for Sentiment Analysis.

Author: [Ivan Bongiorni](https://github.com/IvanBongiorni) - 2022-09-25.

<br>

The structure of this tutorial is loosely based on [this official Keras Notebook](https://keras.io/examples/nlp/text_classification_with_transformer/).

<br>

Let's see how to build and train a Keras model containing a `TransformerLayer` from `maximal`, using the `OriginalTransformerSchedule`.

First, I will import the main libraries I need:
"""

import warnings

import numpy as np
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)

# We need TensorFlow for the model structure:
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, GlobalAveragePooling1D, Dropout, Dense

"""And then we can install `maximal` and import its classes:"""

# !pip install maximal

"""And then we can import `maximal` layers.

The central class in this tutorial is the Transformer layer. In order to include a `TransformerLayer` in a Keras model we need to import a `PositionalEmbedding` layer too. This layer will produce embeddings of words and their relative positions to inform our Attention mechanism.

Additionally, the learning rate schedule of the [original Transformer paper](https://arxiv.org/abs/1706.03762) is added for demonstration purposes.
"""

import maximal
from maximal.layers import PositionalEmbedding, TransformerLayer
from maximal.schedules import OriginalTransformerSchedule
# %%
"""## Load IMDB dataset for Sentiment Analysis"""

vocab_size = 20000
maxlen = 200 # input length

(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)

print(len(x_train), "Training sequences")
print(len(x_val), "Validation sequences")

x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)

print(x_train.shape, y_train.shape)
print(x_val.shape, y_val.shape)

"""## Build the model

Let's first specify some hyperparams about Transformer's hidden size (depth), the number of Attention Heads, and the size of the internal Pointwise Feed-Forward Net:
"""

model_depth = 32
num_heads = 4
ff_dim = 32

"""And then we can specify the model:"""

model = Sequential([
    Input(shape=(maxlen,)),
    PositionalEmbedding(maxlen, vocab_size, model_depth),

    TransformerLayer(model_depth, num_heads, ff_dim),

    GlobalAveragePooling1D(),
    Dropout(0.1),
    Dense(20, activation="relu"),
    Dropout(0.1),
    Dense(2, activation="softmax")
])

"""We are now ready to compile our `model`:"""

# Set learning rate schedule
transformer_schedule = OriginalTransformerSchedule(model_depth)
# check os
import os
# if windows
if os.name == 'nt':
    optimizer = tf.keras.optimizers.Adam(learning_rate=transformer_schedule)
else:
    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=transformer_schedule)



model.compile(
    #optimizer = tf.keras.optimizers.Adam(learning_rate=transformer_schedule),
    optimizer = optimizer,
    loss = tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=["accuracy"]
)

"""## Training
Now the model is ready for training:
"""

pktf.save_model(model,"model.json")

"""
history = model.fit(
    x_train, y_train, batch_size=32, epochs=4, callbacks=my_callbacks, validation_data=(x_val, y_val)
)
"""

epochs=4
for batch_no in range(epochs):
    logs = model.train_on_batch(x_train, y_train)
    pktf.save_weight(model, "model.h5")
    print(f"Epoch {batch_no+1}/{epochs} - loss: {logs[0]} - accuracy: {logs[1]}")